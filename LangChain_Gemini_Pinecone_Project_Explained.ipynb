{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acba7204",
   "metadata": {},
   "source": [
    "\n",
    "# LangChain with Google Generative AI and Pinecone\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project demonstrates my skills in utilizing advanced AI language models and integrations using LangChain, Google Generative AI (Gemini), and Pinecone. This notebook illustrates how to build a LangChain pipeline that interacts with both generative models and vector databases for efficient query embedding, text splitting, and similarity searches.\n",
    "\n",
    "This notebook is designed to demonstrate how we can:\n",
    "- Configure and interact with the `gemini-1.0-pro` model.\n",
    "- Build a conversational AI agent using LangChain with the `gemini-1.0-pro` model.\n",
    "- Create and customize prompt templates for specific AI tasks.\n",
    "- Store and retrieve text embeddings using Pinecone.\n",
    "- Perform similarity search over stored embeddings.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The goal of this notebook is to solve the problem of how to effectively manage large language models and vector embeddings, especially in contexts that require:\n",
    "1. **Text Generation** - generating content, such as explaining concepts like \"autoencoders\" in AI.\n",
    "2. **Embedding Search** - storing documents as embeddings and performing similarity searches.\n",
    "\n",
    "This solution leverages LangChain to integrate various AI tools and frameworks in a modular and extensible way.\n",
    "\n",
    "## Sections\n",
    "\n",
    "- Setting up the environment.\n",
    "- Running a basic query with the Gemini model.\n",
    "- Chat interaction using the the model.\n",
    "- Utilizing LangChain’s prompt templates for text generation.\n",
    "- Performing embedding and similarity searches with Pinecone.\n",
    "- Demonstrating LLMChain for integrating with Python tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc906ca",
   "metadata": {},
   "source": [
    "### 1. Load environment variables\n",
    "\n",
    "This step loads environment variables using `dotenv`. We need API keys for Google Generative AI and Pinecone, which are stored in a `.env` file. This setup ensures secure handling of sensitive information like API keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4187497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe9bf0",
   "metadata": {},
   "source": [
    "### 2. Run basic query with Google Generative AI\n",
    "\n",
    "Here, we use the `google.generativeai` module to configure the API key and run a basic query using the Gemini 1.0 model. This query asks the model to provide interesting information about the University at Buffalo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07dba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run basic query with OpenAI wrapper\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb3751",
   "metadata": {},
   "source": [
    "### 3. Using LangChain to run a query with `gemini-1.5-pro`\n",
    "\n",
    "In this example, we use LangChain’s integration with Google Generative AI to run a query where we ask the model to write a ballad about LangChain. This demonstrates how to interact with advanced language models through LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9876a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI  # type: ignore\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "llm.invoke(\"Write me a ballad about LangChain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2766ed",
   "metadata": {},
   "source": [
    "### 4. Using Gemini-1.5-pro for conversational AI\n",
    "\n",
    "This example shows how we can use Gemini-1.5-pro in a conversational context. A chat system is defined where the model responds to user prompts. In this case, we ask the model to write a Python script that trains a neural network on simulated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"system\", \"You are an expert data scientist\"},\n",
    "    {\"human\", \"Write a Python script that trains a neural network on simulated data \"},\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "\n",
    "print(ai_msg.content, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63415e5a",
   "metadata": {},
   "source": [
    "### 6. Define and use a prompt template\n",
    "\n",
    "Here, we define a `ChatPromptTemplate` where the system is an expert data scientist and the user can ask for an explanation of specific AI concepts. In this case, the model explains the concept of an \"autoencoder\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        {\"system\", \"You are an expert data scientist with an expertise in building deep learning models.\"},\n",
    "        {\"human\", \"Explain the concept of {concept} in a couple of lines\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"concept\": \"autoencoder\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dffdb",
   "metadata": {},
   "source": [
    "### 7. Using LangChain's `LLMChain`\n",
    "\n",
    "This block shows how to use an `LLMChain` to generate responses based on input. The chain runs the model to generate an explanation of an autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79106f8",
   "metadata": {},
   "source": [
    "### 8. Sequential chain with two prompt templates\n",
    "\n",
    "We create a `SimpleSequentialChain` to run two chains consecutively. The first chain generates an explanation of a concept, and the second chain builds on that output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa201bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d44e2a",
   "metadata": {},
   "source": [
    "### 9. Splitting text into chunks using `RecursiveCharacterTextSplitter`\n",
    "\n",
    "To handle large texts, we split them into manageable chunks using LangChain’s `RecursiveCharacterTextSplitter`. This helps us store and process text more efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676757a7",
   "metadata": {},
   "source": [
    "### 10. Embedding text and setting up Pinecone for vector storage\n",
    "\n",
    "We create embeddings for the text chunks and set up a Pinecone index for storing the embeddings. The embeddings are stored in a vector database, which can be used for fast similarity searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761be273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index_name = \"knowledge-from-llms\"\n",
    "index = pc.Index(index_name)\n",
    "vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
    "\n",
    "vector_store.add_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7db5d",
   "metadata": {},
   "source": [
    "### 11. Similarity search with Pinecone\n",
    "\n",
    "After storing the embeddings in Pinecone, we perform a similarity search to find the most relevant text based on a query. In this case, we ask, \"What is magical about an autoencoder?\" and retrieve the most similar document from our vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is magical about an autoencoder?\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "result = vector_store.similarity_search(query, 1)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0dfe91",
   "metadata": {},
   "source": [
    "### 12. Running Python commands with LangChain and a REPL\n",
    "\n",
    "In this section, we integrate a Python REPL tool with LangChain. This allows us to send commands to the model and, if necessary, execute Python code. For example, we ask the model to calculate the square of 7 and find the roots of a quadratic function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.experimental.tools import PythonREPLTool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize the LLM (Google Gemini Pro)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\")\n",
    "python_repl = PythonREPLTool()\n",
    "\n",
    "def run_agent(input_text):\n",
    "    response = llm_chain.run(input_text)\n",
    "    if \"calculate\" in input_text.lower():\n",
    "        python_result = python_repl.run(response)\n",
    "        return python_result\n",
    "    return response\n",
    "\n",
    "command = \"Calculate the square of 7\"\n",
    "result = run_agent(command)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
